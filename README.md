# DailyBestPaper
记录每日精选的论文，当然也不是每日啦，就当作一个记录的地方吧~~~

### 2022-02-06
PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts
https://arxiv.org/pdf/2202.01279.pdf

mSLAM: Massively multilingual joint pre-training for speech and text
https://arxiv.org/abs/2202.01374


### 2022-02-04
Pre-Trained Language Transformers are Universal Image Classifiers
https://arxiv.org/abs/2201.10182

Improving the fusion of acoustic and text representations in RNN-T
https://arxiv.org/abs/2201.10240

A Survey on Retrieval-Augmented Text Generation
https://arxiv.org/pdf/2202.01110.pdf

Natural Language to Code Using Transformers
https://arxiv.org/abs/2202.00367

Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning
https://arxiv.org/pdf/2202.00535.pdf

Improving BERT-based Query-by-Document Retrieval with Multi-Task Optimization
https://arxiv.org/abs/2202.00373

Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey
https://arxiv.org/abs/2201.12438

A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis
https://arxiv.org/abs/2201.12549

Unsupervised Summarization with Customized Granularities
https://arxiv.org/pdf/2201.12502.pdf

Clinical-Longformer and Clinical-BigBird
https://arxiv.org/abs/2201.11838

PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings
https://arxiv.org/pdf/2201.12093.pdf

Protum: A New Method For Prompt Tuning Based on "[MASK]"
https://arxiv.org/pdf/2201.12109.pdf

DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence
https://arxiv.org/pdf/2201.11176.pdf

### 2022-02-02
Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation
https://arxiv.org/pdf/2201.08702.pdf

Context-Tuning: Learning Contextualized Prompts for Natural Language Generation
https://arxiv.org/pdf/2201.08670.pdf

Black-box Prompt Learning for Pre-trained Language Models
https://arxiv.org/pdf/2201.08531.pdf

SciBERTSUM: Extractive Summarization for Scientific Documents
https://arxiv.org/pdf/2201.08495.pdf

ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization
https://arxiv.org/pdf/2201.06910.pdf

CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks
https://arxiv.org/pdf/2201.05729.pdf

Double Retrieval and Ranking for Accurate Question Answering
https://arxiv.org/pdf/2201.05981.pdf

Improving Biomedical Information Retrieval with Neural Retrievers
https://arxiv.org/pdf/2201.07745.pdf

PromptBERT: Improving BERT Sentence Embeddings with Prompts
https://arxiv.org/pdf/2201.04337.pdf

Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems
https://arxiv.org/pdf/2201.05767.pdf

The Dark Side of the Language: Pre-trained Transformers in the DarkNet
https://arxiv.org/pdf/2201.05613.pdf

AugLy: Data Augmentations for Robustness
https://arxiv.org/pdf/2201.06494.pdf

Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer
https://arxiv.org/pdf/2201.05411.pdf

