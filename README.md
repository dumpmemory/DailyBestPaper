# DailyBestPaper
记录每日精选的论文，当然也不是每日啦，就当作一个记录的地方吧~~~
- 每周至少3天刷arxiv订阅邮件！
- 将感兴趣或不错的论文记录下来！
- 精读过的文章标记"⭐"！
- 督促自己每天都有进步！go！go！加油！

#### 2022-04-27
sigir2022论文
##### 检索排序相关
RankFlow: Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows

Incorporating Explicit Knowledge in Pre-trained Language Models for Passage Re-ranking

IR Evaluation and Learning in the Presence of Forbidden Documents

Incorporating Retrieval Information into the Truncation of Ranking Lists in the Legal Domain

Webformer: Pre-training with Web Pages for Information Retrieval

Automatic Expert Selection for Multi-Scenario and Multi-Task Search

CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos

H-ERNIE: A Hierarchical Multi-Granularity Pre-Trained Language Model for Chinese Search Engine

Interpreting Patient Descriptions using Distantly Supervised Similar Case Retrieval

Axiomatically Regularized Pre-training for Ad hoc Search

Entity-aware Transformers for Entity Search

Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction

Faster Learned Sparse Retrieval with Guided Traversal

P^3 Ranker: Mitigating the Gaps between Pre-training and Ranking Fine-tuning with Prompt-based Learning and Pre-finetuning

Curriculum Learning for Dense Retrieval Distillation

Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings

From Cluster Ranking to Document Ranking

How does Feedback Signal Quality Impact Effectiveness of Pseudo Relevance Feedback for Passage Retrieval

Improving Contrastive Learning of Sentence Embeddings with Case-Augmented Positives and Retrieved Negatives

GERE: Generative Evidence Retrieval for Fact Verification

Learning to Rank Knowledge Sub-Graph Nodes for Entity Retrieval

BERT-based Dense Intra-ranking and Contextualized Late Interaction via Multi-task Learning for Long Document Retrieval

From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective

Long Document Re-ranking with Modular Re-ranker

Unsupervised Dataset Generation for Information Retrieval

On Optimizing Top-K Metrics for Neural Ranking Models

Alignment Rationale for Query-Document

To interpolate or not to interpolate: PRF, Dense Retrievers and BM25

C3: Continued Pretraining with Contrastive Weak Supervision for Cross Language Ad-Hoc Retrieval
##### 小样本
Curriculum Contrastive Context Denoising for Few-shot Conversational Dense Retrieval

Recognizing Medical Search Query Intent by Few-shot Learning

Zero-shot Query Contextualization for Conversational Search

Relation-Guided Few-Shot Relational Triple Extraction

Multi-labels Masked Language Modeling on Zero-shot Code-switched Sentiment Analysis
##### 多模态相关
Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion

Learn from Unlabeled Videos for Near-duplicate Video Retrieval

Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation

V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation

CenterCLIP: Token Clustering for Efficient Text-Video Retrieval

Multimodal Entity Linking with Gated Hierarchical Fusion and Contrastive Training

Bit-aware Semantic Transformer Hashing for Multi-modal Retrieval

Multimodal Disentanglement Variational AutoEncoders for Zero-Shot Cross-Modal Retrieval

Animating Images to transfer CLIP for Video-Text Retrieval

Image-Text Retrieval via Contrastive Learning with Auxiliary Generative Features and Support-set Regularization

Cross-Probe BERT for Fast Cross-Modal Search
##### 生成

Mutual Disentanglement Learning for Joint Fine-Grained Sentiment Classification and Controllable Text Generation

Target-aware Abstractive Related Work Generation with Contrastive Learning

Generating Clarifying Questions with Web Search Results

Task-Oriented Dialogue System as Natural Language Generation

Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval

##### 摘要
ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement

On Extractive Summarization for Profile-centric Neural Expert Search in Academia

QSG Transformer: Transformer with Query-Attentive Semantic Graph for Query-Focused Summarization

MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization

Lightweight Meta-Learning for Low-Resource Abstractive Summarization

Extractive Elementary Discourse Units for Improving Abstractive Summarization

Summarizing Legal Regulatory Documents using Transformers
##### 文本表征

Structure and Semantics Preserving Document Representations

Unsupervised Belief Representation Learning with Information-Theoretic Variational Graph Auto-Encoders

CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval

BERT-ER: Query-Specific BERT Entity Representations for Entity Ranking

Adaptable Text Matching via Meta-Weight Regulator

Re-weighting Negative Samples for Model-Agnostic Matching

Learned Token Pruning in Contextualized Late Interaction over BERT (ColBERT)
##### 问答

PTAU: Prompt Tuning for Attributing Unanswerable Questions

DGQAN: Dual Graph Question-Answer Attention Networks for Answer Selection

Conversational Question Answering on Heterogeneous Sources

QUASER: Question Answering with Scalable Extractive Rationalization

Detecting Frozen Phrases in Open-Domain Question Answering
##### 其他
Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning

LoL: A Comparative Regularization Loss over Query Reformulation Losses for Pseudo-Relevance Feedback

A Dual-Expert Framework for Event Argument Extraction

Aspect Feature Distillation and Enhancement Network for Aspect-based Sentiment Analysis

IAOTP: An Interactive End-to-End Solution for Aspect-Opinion Term Pairs Extraction

Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning

Training Entire-Space Models for Target-oriented Opinion Words Extraction

A Robust Computerized Adaptive Testing Approach in Educational Question Retrieval

Hierarchical Task-aware Multi-Head Attention Network

Enhancing Event-Level Sentiment Analysis with Structured Arguments

End-to-end Distantly Supervised Information Extraction with Retrieval Augmentation
##### 资源
A Dataset for Sentence Retrieval for Open-Ended Dialogues

ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues

Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieva

#### 2022-02-09
[LST: Lexicon-Guided Self-Training for Few-Shot Text Classification](https://arxiv.org/pdf/2202.02566.pdf)

[Comparison and Combination of Sentence Embeddings Derived from Different Supervision Signals](https://arxiv.org/pdf/2202.02990.pdf)

[To Tune or Not To Tune? Zero-shot Models for Legal Case Entailment](https://arxiv.org/pdf/2202.03120.pdf)


#### 2022-02-08
[Zero-Shot Aspect-Based Sentiment Analysis](https://arxiv.org/pdf/2202.01924.pdf)

[A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications](https://arxiv.org/pdf/2202.02013.pdf)

[From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer](https://arxiv.org/pdf/2202.02113.pdf)


#### 2022-02-06
[PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts](https://arxiv.org/pdf/2202.01279.pdf)⭐

[mSLAM: Massively multilingual joint pre-training for speech and text](https://arxiv.org/abs/2202.01374)


#### 2022-02-04
[Pre-Trained Language Transformers are Universal Image Classifiers](https://arxiv.org/abs/2201.10182)

[Improving the fusion of acoustic and text representations in RNN-T](https://arxiv.org/abs/2201.10240)

[A Survey on Retrieval-Augmented Text Generation](https://arxiv.org/pdf/2202.01110.pdf)⭐

[Natural Language to Code Using Transformers](https://arxiv.org/abs/2202.00367)

[Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning](https://arxiv.org/pdf/2202.00535.pdf)

[Improving BERT-based Query-by-Document Retrieval with Multi-Task Optimization](https://arxiv.org/abs/2202.00373)

[Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey](https://arxiv.org/abs/2201.12438)

[A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2201.12549)

[Unsupervised Summarization with Customized Granularities](https://arxiv.org/pdf/2201.12502.pdf)

[Clinical-Longformer and Clinical-BigBird](https://arxiv.org/abs/2201.11838)

[PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings](https://arxiv.org/pdf/2201.12093.pdf)

[Protum: A New Method For Prompt Tuning Based on "[MASK]"](https://arxiv.org/pdf/2201.12109.pdf)⭐

[DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence](https://arxiv.org/pdf/2201.11176.pdf)


#### 2022-02-02
[Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation](https://arxiv.org/pdf/2201.08702.pdf)

[Context-Tuning: Learning Contextualized Prompts for Natural Language Generation](https://arxiv.org/pdf/2201.08670.pdf)

[Black-box Prompt Learning for Pre-trained Language Models](https://arxiv.org/pdf/2201.08531.pdf)

[SciBERTSUM: Extractive Summarization for Scientific Documents](https://arxiv.org/pdf/2201.08495.pdf)⭐

[ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization](https://arxiv.org/pdf/2201.06910.pdf)⭐

[CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks](https://arxiv.org/pdf/2201.05729.pdf)

[Double Retrieval and Ranking for Accurate Question Answering](https://arxiv.org/pdf/2201.05981.pdf)

[Improving Biomedical Information Retrieval with Neural Retrievers](https://arxiv.org/pdf/2201.07745.pdf)

[PromptBERT: Improving BERT Sentence Embeddings with Prompts](https://arxiv.org/pdf/2201.04337.pdf)⭐

[Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems](https://arxiv.org/pdf/2201.05767.pdf)

[The Dark Side of the Language: Pre-trained Transformers in the DarkNet](https://arxiv.org/pdf/2201.05613.pdf)

[AugLy: Data Augmentations for Robustness](https://arxiv.org/pdf/2201.06494.pdf)⭐

[Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer](https://arxiv.org/pdf/2201.05411.pdf)

